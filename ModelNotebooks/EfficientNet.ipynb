{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DtsATYJjeeFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cbd1e7-5001-4b22-95fd-df7dfa501fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16426 sha256=5e328eae7b67aaff0ae5308d9aaed8f431e6ef1195f6be11a1654381ee512db5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import torch.optim as optim\n",
        "import shutil\n",
        "import random\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "OHaIlBm9fyBt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "4m3_5YUdofLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#Connecting google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPpvr980n6sm",
        "outputId": "335d2781-1422-4a4f-fe63-4ae575c8914a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dataset_dir = \"/content/drive/MyDrive/FinalYearProject/ClassificationModels/Dataset\" #Original Dataset Location\n",
        "output_dir = \"/content/drive/MyDrive/FinalYearProject/ClassificationModels/SplitDataset\"  #Output Dataset Location"
      ],
      "metadata": {
        "id": "dC69tC57omh5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Splits for the dataset train/test/val\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "split_ratios = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15}"
      ],
      "metadata": {
        "id": "CUGsv5ur9_gG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create split directories\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(output_dir, split), exist_ok=True)"
      ],
      "metadata": {
        "id": "S5Z6UEcFA5H5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Organsisng dataset folders\n",
        "for record_class in os.listdir(base_dataset_dir):\n",
        "    class_path = os.path.join(base_dataset_dir, record_class)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    images = os.listdir(class_path)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    #Compute split sizes\n",
        "    num_train = int(len(images) * split_ratios[\"train\"])\n",
        "    num_val = int(len(images) * split_ratios[\"val\"])\n",
        "\n",
        "    train_images = images[:num_train]\n",
        "    val_images = images[num_train:num_train + num_val]\n",
        "    test_images = images[num_train + num_val:]\n",
        "\n",
        "    #Move images to respective folders\n",
        "    for split, split_images in zip(splits, [train_images, val_images, test_images]):\n",
        "        split_class_path = os.path.join(output_dir, split, record_class)\n",
        "        os.makedirs(split_class_path, exist_ok=True)\n",
        "\n",
        "        for img in split_images:\n",
        "            src = os.path.join(class_path, img)\n",
        "            dst = os.path.join(split_class_path, img)\n",
        "            shutil.copy(src, dst)"
      ],
      "metadata": {
        "id": "6tZOTYL4BajR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for converting images to the same format\n",
        "def convert_to_jpg_recursive(directory):\n",
        "    \"\"\" Recursively converts all images in the dataset to JPG format and removes originals\"\"\"\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "\n",
        "            #Get file extension\n",
        "            ext = filename.split(\".\")[-1].lower()\n",
        "            if ext in [\"jpg\", \"jpeg\"]:\n",
        "                continue  #Skip already JPG images\n",
        "\n",
        "            try:\n",
        "                #Open image\n",
        "                with Image.open(file_path) as img:\n",
        "                    img = img.convert(\"RGB\")  #Convert to RGB\n",
        "\n",
        "                    #Save new file as JPG\n",
        "                    new_filename = f\"{os.path.splitext(filename)[0]}.jpg\"\n",
        "                    new_path = os.path.join(root, new_filename)\n",
        "                    img.save(new_path, \"JPEG\", quality=95)\n",
        "\n",
        "                #Verify successful conversion before deleting old file\n",
        "                if os.path.exists(new_path):\n",
        "                    os.remove(file_path)  #Remove original file\n",
        "                    print(f\"Converted & Deleted: {filename} - {new_filename}\")\n",
        "                else:\n",
        "                    print(f\"Failed to convert {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting {filename}: {e}\")"
      ],
      "metadata": {
        "id": "7-y6m8YQoj9m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting all images to the same format for consistency\n",
        "convert_to_jpg_recursive(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxSl94vOGTd7",
        "outputId": "b1c11c85-c5a2-4322-ab50-a1932a5b67cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted & Deleted: wicked11.webp - wicked11.jpg\n",
            "Error converting taste7.avif: cannot identify image file '/content/drive/MyDrive/FinalYearProject/ClassificationModels/SplitDataset/train/TasteCovers/taste7.avif'\n",
            "Converted & Deleted: taste17.webp - taste17.jpg\n",
            "Converted & Deleted: taste6.webp - taste6.jpg\n",
            "Converted & Deleted: taste2.webp - taste2.jpg\n",
            "Converted & Deleted: taste8.webp - taste8.jpg\n",
            "Converted & Deleted: midnights12.webp - midnights12.jpg\n",
            "Converted & Deleted: midnights8.webp - midnights8.jpg\n",
            "Converted & Deleted: midnights3.webp - midnights3.jpg\n",
            "Converted & Deleted: whitney18.webp - whitney18.jpg\n",
            "Error converting whitney1.avif: cannot identify image file '/content/drive/MyDrive/FinalYearProject/ClassificationModels/SplitDataset/train/WhitneyCovers/whitney1.avif'\n",
            "Converted & Deleted: whitney3.png - whitney3.jpg\n",
            "Converted & Deleted: whitney8.webp - whitney8.jpg\n",
            "Converted & Deleted: whitney12.webp - whitney12.jpg\n",
            "Converted & Deleted: rumours15.webp - rumours15.jpg\n",
            "Converted & Deleted: rumours2.webp - rumours2.jpg\n",
            "Converted & Deleted: taste14.webp - taste14.jpg\n",
            "Converted & Deleted: midnights7.png - midnights7.jpg\n",
            "Converted & Deleted: rumours4.webp - rumours4.jpg\n",
            "Converted & Deleted: midnights9.webp - midnights9.jpg\n",
            "Converted & Deleted: whitney14.webp - whitney14.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentations and Data Loading"
      ],
      "metadata": {
        "id": "JDqc4veHNvyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, RandomRotate90, ShiftScaleRotate,\n",
        "    RandomBrightnessContrast, RGBShift, Resize, Compose\n",
        ")\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "hCp0mNFiNza1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define augmentations using Albumentations\n",
        "augmentation_pipeline = Compose([\n",
        "    Resize(224, 224, p=1.0),\n",
        "    HorizontalFlip(p=0.5),\n",
        "    VerticalFlip(p=0.2),\n",
        "    RandomRotate90(p=0.5),\n",
        "    ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
        "    RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFmiLr7dYKo8",
        "outputId": "e376a2ee-5258-407e-9d57-49e6e98eeb39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_images(original_dataset_path, augmented_dataset_path, num_augments=10):\n",
        "    \"\"\"\n",
        "    Augments images in the train set and copies val/test sets as they are.\n",
        "\n",
        "    :param original_dataset_path: Path to the original dataset (SplitDataset).\n",
        "    :param augmented_dataset_path: Path to save the augmented dataset.\n",
        "    :param num_augments: Number of augmented images per original (default: 10).\n",
        "    \"\"\"\n",
        "    # Create necessary directories\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        split_path = os.path.join(augmented_dataset_path, split)\n",
        "        os.makedirs(split_path, exist_ok=True)\n",
        "\n",
        "        for class_folder in os.listdir(os.path.join(original_dataset_path, split)):\n",
        "            class_input_path = os.path.join(original_dataset_path, split, class_folder)\n",
        "            class_output_path = os.path.join(split_path, class_folder)\n",
        "            os.makedirs(class_output_path, exist_ok=True)\n",
        "\n",
        "            for img_name in os.listdir(class_input_path):\n",
        "                img_path = os.path.join(class_input_path, img_name)\n",
        "                image = cv2.imread(img_path)\n",
        "\n",
        "                if image is None:\n",
        "                    print(f\"Warning: Could not read {img_path}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                #Train Set Apply Augmentations\n",
        "                if split == \"train\":\n",
        "                    for i in range(num_augments):\n",
        "                        augmented = augmentation_pipeline(image=image)[\"image\"]\n",
        "                        aug_img_name = f\"{os.path.splitext(img_name)[0]}_aug_{i}.jpg\"\n",
        "                        aug_img_path = os.path.join(class_output_path, aug_img_name)\n",
        "                        cv2.imwrite(aug_img_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                #Val/Test Sets no augmentation\n",
        "                else:\n",
        "                    shutil.copy(img_path, os.path.join(class_output_path, img_name))"
      ],
      "metadata": {
        "id": "QnnvWqzfYbhZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output path for the augmented images\n",
        "augmented_output_path = \"/content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset\""
      ],
      "metadata": {
        "id": "fNUXXGWubzu8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmenting images\n",
        "augment_images(output_dir, augmented_output_path, num_augments=10)"
      ],
      "metadata": {
        "id": "thhTJT3ocWw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing For Training Model"
      ],
      "metadata": {
        "id": "TINOZJgSTb1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Function for counting the amount of images for testign purposes\n",
        "def count_images(dataset_path):\n",
        "    \"\"\"\n",
        "    Counts and prints the number of images in each class folder for train, val, and test sets.\n",
        "\n",
        "    :param dataset_path: Path to the dataset (AugmentedDataset).\n",
        "    \"\"\"\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        split_path = os.path.join(dataset_path, split)\n",
        "        print(f\"\\nChecking '{split}' dataset...\")\n",
        "\n",
        "        if not os.path.exists(split_path):\n",
        "            print(f\"Error: {split_path} does not exist!\")\n",
        "            continue\n",
        "\n",
        "        total_images = 0\n",
        "\n",
        "        for class_folder in sorted(os.listdir(split_path)):\n",
        "            class_path = os.path.join(split_path, class_folder)\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue\n",
        "\n",
        "            num_images = len([f for f in os.listdir(class_path) if f.endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
        "            total_images += num_images\n",
        "\n",
        "            print(f\"{class_folder}: {num_images} images\")\n",
        "\n",
        "        print(f\"Total in '{split}': {total_images} images\")\n",
        "\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset/\"\n",
        "count_images(dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEzYNEL9n0jh",
        "outputId": "9ae4c0ba-47d9-455e-f934-7f5a3beff744"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking 'train' dataset...\n",
            "MidnightsCovers: 140 images\n",
            "RumoursCovers: 140 images\n",
            "TasteCovers: 140 images\n",
            "WhitneyCovers: 140 images\n",
            "WickedCovers: 140 images\n",
            "Total in 'train': 700 images\n",
            "\n",
            "Checking 'val' dataset...\n",
            "MidnightsCovers: 3 images\n",
            "RumoursCovers: 3 images\n",
            "TasteCovers: 3 images\n",
            "WhitneyCovers: 3 images\n",
            "WickedCovers: 3 images\n",
            "Total in 'val': 15 images\n",
            "\n",
            "Checking 'test' dataset...\n",
            "MidnightsCovers: 3 images\n",
            "RumoursCovers: 3 images\n",
            "TasteCovers: 3 images\n",
            "WhitneyCovers: 3 images\n",
            "WickedCovers: 3 images\n",
            "Total in 'test': 15 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define transformations for the train, val, and test sets\n",
        "# Data augmentation for the training dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),      #Randomly crop and resize image\n",
        "    transforms.RandomHorizontalFlip(),      #Random horizontal flip\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#For validation and test, no augmentation, just normalization\n",
        "transform_test_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "6b-chdfHXFZO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the datasets from the folders\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset/train', transform=transform_train)\n",
        "val_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset/augmented_val', transform=transform_test_val)\n",
        "test_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset/augmented_test', transform=transform_test_val)"
      ],
      "metadata": {
        "id": "2BLy5MJlXMUM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Creating DataLoader instances for each dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Check the number of samples in each dataset\n",
        "print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_loader.dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_loader.dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnn090oXXVjF",
        "outputId": "d118142f-2467-4989-af54-de88e8c32c5d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 700\n",
            "Validation dataset size: 150\n",
            "Test dataset size: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Early stopping in training loop\n",
        "early_stopping = EarlyStopping(patience=3, delta=0.001)"
      ],
      "metadata": {
        "id": "8UJ6jX4orZ1W"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "1cI1zHpBYa7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "#Load the pre-trained EfficientNet model\n",
        "model = models.efficientnet_b0(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qiBu4cSYZdt",
        "outputId": "ae21985d-6b97-463a-85a4-6dc7880e02d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 152MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze feature extractor layers\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "ep9aLETRnLNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modifying the final fully connected layer to match the number of classes\n",
        "num_ftrs = model.classifier[1].in_features   #Number of input features to the final layer\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(num_ftrs, 5) #Matches numeber of classes - 5\n",
        ")"
      ],
      "metadata": {
        "id": "3fSsJWJXY1eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Send the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "9XK1KX0VZB1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "pu2R3beyZ9SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "#Define the training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Started\")\n",
        "        model.train()  #Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            #Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            #Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            #Show progress every few batches\n",
        "            if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_loader) - 1:\n",
        "                print(f\"  Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        #Epoch summary\n",
        "        train_acc = correct / total\n",
        "        print(f\"\\nEpoch {epoch+1} Complete - Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc*100:.2f}%\")\n",
        "\n",
        "        #Validate the model\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # if early_stopping(val_acc):\n",
        "        #   print(f\"Stopping early at epoch {epoch+1}\")\n",
        "        #   break\n",
        "\n",
        "        #Save the best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "    print(\"\\nTraining complete\")"
      ],
      "metadata": {
        "id": "-qE0peScaEHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()  #Set model to evaluation mode\n",
        "    y_true, y_pred = [], []  #Lists to store true and predicted labels\n",
        "\n",
        "    with torch.no_grad():  #Disable gradient computation for validation\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            #Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            #Get predicted class by selecting the class with the highest probability\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            #Convert to numpy arrays and extend the lists\n",
        "            y_true.extend(labels.cpu().numpy())  #onvert true labels to numpy\n",
        "            y_pred.extend(predicted.cpu().numpy())  #Convert predicted labels to numpy\n",
        "\n",
        "    #Call classification_report only if y_true and y_pred are valid\n",
        "    if y_true and y_pred:\n",
        "        print(classification_report(y_true, y_pred))\n",
        "    else:\n",
        "        print(\"Error: y_true or y_pred is empty.\")\n",
        "\n",
        "    accuracy = sum([1 for true, pred in zip(y_true, y_pred) if true == pred]) / len(y_true)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "eTPrNJ0WaJ4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4f5yIpbaOee",
        "outputId": "39c5ab4a-0cc1-4e0c-fbd1-f1deb36d9897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15 Started\n",
            "  Batch [10/44] - Loss: 1.6367\n",
            "  Batch [20/44] - Loss: 1.6582\n",
            "  Batch [30/44] - Loss: 1.5119\n",
            "  Batch [40/44] - Loss: 1.4928\n",
            "  Batch [44/44] - Loss: 1.5263\n",
            "\n",
            "Epoch 1 Complete - Loss: 1.5877, Train Accuracy: 24.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.60      0.46        30\n",
            "           1       0.48      0.37      0.42        30\n",
            "           2       0.29      0.20      0.24        30\n",
            "           3       0.44      0.40      0.42        30\n",
            "           4       0.32      0.33      0.33        30\n",
            "\n",
            "    accuracy                           0.38       150\n",
            "   macro avg       0.38      0.38      0.37       150\n",
            "weighted avg       0.38      0.38      0.37       150\n",
            "\n",
            "Validation Accuracy: 38.00%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 2/15 Started\n",
            "  Batch [10/44] - Loss: 1.4760\n",
            "  Batch [20/44] - Loss: 1.4621\n",
            "  Batch [30/44] - Loss: 1.4235\n",
            "  Batch [40/44] - Loss: 1.4207\n",
            "  Batch [44/44] - Loss: 1.4858\n",
            "\n",
            "Epoch 2 Complete - Loss: 1.4697, Train Accuracy: 41.86%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.57      0.49        30\n",
            "           1       0.45      0.60      0.51        30\n",
            "           2       0.36      0.27      0.31        30\n",
            "           3       0.69      0.37      0.48        30\n",
            "           4       0.39      0.43      0.41        30\n",
            "\n",
            "    accuracy                           0.45       150\n",
            "   macro avg       0.47      0.45      0.44       150\n",
            "weighted avg       0.47      0.45      0.44       150\n",
            "\n",
            "Validation Accuracy: 44.67%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 3/15 Started\n",
            "  Batch [10/44] - Loss: 1.3626\n",
            "  Batch [20/44] - Loss: 1.3807\n",
            "  Batch [30/44] - Loss: 1.2885\n",
            "  Batch [40/44] - Loss: 1.2471\n",
            "  Batch [44/44] - Loss: 1.2419\n",
            "\n",
            "Epoch 3 Complete - Loss: 1.3490, Train Accuracy: 59.86%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.63      0.57        30\n",
            "           1       0.57      0.77      0.66        30\n",
            "           2       0.55      0.40      0.46        30\n",
            "           3       0.79      0.50      0.61        30\n",
            "           4       0.50      0.53      0.52        30\n",
            "\n",
            "    accuracy                           0.57       150\n",
            "   macro avg       0.58      0.57      0.56       150\n",
            "weighted avg       0.58      0.57      0.56       150\n",
            "\n",
            "Validation Accuracy: 56.67%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 4/15 Started\n",
            "  Batch [10/44] - Loss: 1.2672\n",
            "  Batch [20/44] - Loss: 1.3428\n",
            "  Batch [30/44] - Loss: 1.2545\n",
            "  Batch [40/44] - Loss: 1.0960\n",
            "  Batch [44/44] - Loss: 1.1638\n",
            "\n",
            "Epoch 4 Complete - Loss: 1.2300, Train Accuracy: 75.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.60      0.63        30\n",
            "           1       0.58      0.70      0.64        30\n",
            "           2       0.62      0.53      0.57        30\n",
            "           3       0.88      0.50      0.64        30\n",
            "           4       0.55      0.80      0.65        30\n",
            "\n",
            "    accuracy                           0.63       150\n",
            "   macro avg       0.66      0.63      0.63       150\n",
            "weighted avg       0.66      0.63      0.63       150\n",
            "\n",
            "Validation Accuracy: 62.67%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 5/15 Started\n",
            "  Batch [10/44] - Loss: 1.0622\n",
            "  Batch [20/44] - Loss: 1.1055\n",
            "  Batch [30/44] - Loss: 1.1109\n",
            "  Batch [40/44] - Loss: 1.0856\n",
            "  Batch [44/44] - Loss: 0.9790\n",
            "\n",
            "Epoch 5 Complete - Loss: 1.1280, Train Accuracy: 78.57%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.60      0.68        30\n",
            "           1       0.58      0.83      0.68        30\n",
            "           2       0.62      0.70      0.66        30\n",
            "           3       1.00      0.43      0.60        30\n",
            "           4       0.65      0.80      0.72        30\n",
            "\n",
            "    accuracy                           0.67       150\n",
            "   macro avg       0.73      0.67      0.67       150\n",
            "weighted avg       0.73      0.67      0.67       150\n",
            "\n",
            "Validation Accuracy: 67.33%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 6/15 Started\n",
            "  Batch [10/44] - Loss: 0.9883\n",
            "  Batch [20/44] - Loss: 0.9771\n",
            "  Batch [30/44] - Loss: 0.9931\n",
            "  Batch [40/44] - Loss: 1.0758\n",
            "  Batch [44/44] - Loss: 0.8514\n",
            "\n",
            "Epoch 6 Complete - Loss: 1.0293, Train Accuracy: 85.14%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.70      0.72        30\n",
            "           1       0.65      0.93      0.77        30\n",
            "           2       0.75      0.70      0.72        30\n",
            "           3       1.00      0.47      0.64        30\n",
            "           4       0.65      0.80      0.72        30\n",
            "\n",
            "    accuracy                           0.72       150\n",
            "   macro avg       0.76      0.72      0.71       150\n",
            "weighted avg       0.76      0.72      0.71       150\n",
            "\n",
            "Validation Accuracy: 72.00%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 7/15 Started\n",
            "  Batch [10/44] - Loss: 0.8520\n",
            "  Batch [20/44] - Loss: 1.0352\n",
            "  Batch [30/44] - Loss: 0.9434\n",
            "  Batch [40/44] - Loss: 1.0291\n",
            "  Batch [44/44] - Loss: 0.9165\n",
            "\n",
            "Epoch 7 Complete - Loss: 0.9645, Train Accuracy: 84.71%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.63      0.72        30\n",
            "           1       0.61      0.90      0.73        30\n",
            "           2       0.59      0.73      0.66        30\n",
            "           3       1.00      0.43      0.60        30\n",
            "           4       0.67      0.73      0.70        30\n",
            "\n",
            "    accuracy                           0.69       150\n",
            "   macro avg       0.74      0.69      0.68       150\n",
            "weighted avg       0.74      0.69      0.68       150\n",
            "\n",
            "Validation Accuracy: 68.67%\n",
            "\n",
            "Epoch 8/15 Started\n",
            "  Batch [10/44] - Loss: 0.9495\n",
            "  Batch [20/44] - Loss: 0.8698\n",
            "  Batch [30/44] - Loss: 1.0092\n",
            "  Batch [40/44] - Loss: 1.0098\n",
            "  Batch [44/44] - Loss: 1.1355\n",
            "\n",
            "Epoch 8 Complete - Loss: 0.9277, Train Accuracy: 85.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.60      0.68        30\n",
            "           1       0.66      0.90      0.76        30\n",
            "           2       0.62      0.70      0.66        30\n",
            "           3       1.00      0.40      0.57        30\n",
            "           4       0.60      0.80      0.69        30\n",
            "\n",
            "    accuracy                           0.68       150\n",
            "   macro avg       0.73      0.68      0.67       150\n",
            "weighted avg       0.73      0.68      0.67       150\n",
            "\n",
            "Validation Accuracy: 68.00%\n",
            "\n",
            "Epoch 9/15 Started\n",
            "  Batch [10/44] - Loss: 0.8284\n",
            "  Batch [20/44] - Loss: 0.8155\n",
            "  Batch [30/44] - Loss: 1.0241\n",
            "  Batch [40/44] - Loss: 0.7330\n",
            "  Batch [44/44] - Loss: 0.7581\n",
            "\n",
            "Epoch 9 Complete - Loss: 0.8476, Train Accuracy: 88.71%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.63      0.69        30\n",
            "           1       0.57      0.97      0.72        30\n",
            "           2       0.71      0.57      0.63        30\n",
            "           3       0.93      0.43      0.59        30\n",
            "           4       0.69      0.83      0.76        30\n",
            "\n",
            "    accuracy                           0.69       150\n",
            "   macro avg       0.73      0.69      0.68       150\n",
            "weighted avg       0.73      0.69      0.68       150\n",
            "\n",
            "Validation Accuracy: 68.67%\n",
            "\n",
            "Epoch 10/15 Started\n",
            "  Batch [10/44] - Loss: 0.7865\n",
            "  Batch [20/44] - Loss: 0.7347\n",
            "  Batch [30/44] - Loss: 0.8118\n",
            "  Batch [40/44] - Loss: 0.9493\n",
            "  Batch [44/44] - Loss: 0.6962\n",
            "\n",
            "Epoch 10 Complete - Loss: 0.7915, Train Accuracy: 91.71%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.60      0.73        30\n",
            "           1       0.56      0.90      0.69        30\n",
            "           2       0.58      0.73      0.65        30\n",
            "           3       1.00      0.40      0.57        30\n",
            "           4       0.73      0.80      0.76        30\n",
            "\n",
            "    accuracy                           0.69       150\n",
            "   macro avg       0.76      0.69      0.68       150\n",
            "weighted avg       0.76      0.69      0.68       150\n",
            "\n",
            "Validation Accuracy: 68.67%\n",
            "\n",
            "Epoch 11/15 Started\n",
            "  Batch [10/44] - Loss: 0.7735\n",
            "  Batch [20/44] - Loss: 0.7120\n",
            "  Batch [30/44] - Loss: 0.6939\n",
            "  Batch [40/44] - Loss: 0.6597\n",
            "  Batch [44/44] - Loss: 0.7217\n",
            "\n",
            "Epoch 11 Complete - Loss: 0.7363, Train Accuracy: 91.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.63      0.73        30\n",
            "           1       0.59      1.00      0.74        30\n",
            "           2       0.65      0.67      0.66        30\n",
            "           3       1.00      0.40      0.57        30\n",
            "           4       0.71      0.80      0.75        30\n",
            "\n",
            "    accuracy                           0.70       150\n",
            "   macro avg       0.76      0.70      0.69       150\n",
            "weighted avg       0.76      0.70      0.69       150\n",
            "\n",
            "Validation Accuracy: 70.00%\n",
            "\n",
            "Epoch 12/15 Started\n",
            "  Batch [10/44] - Loss: 0.6379\n",
            "  Batch [20/44] - Loss: 0.7208\n",
            "  Batch [30/44] - Loss: 0.6806\n",
            "  Batch [40/44] - Loss: 0.5900\n",
            "  Batch [44/44] - Loss: 0.7376\n",
            "\n",
            "Epoch 12 Complete - Loss: 0.6698, Train Accuracy: 92.71%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.60      0.73        30\n",
            "           1       0.60      0.97      0.74        30\n",
            "           2       0.72      0.70      0.71        30\n",
            "           3       0.94      0.53      0.68        30\n",
            "           4       0.70      0.87      0.78        30\n",
            "\n",
            "    accuracy                           0.73       150\n",
            "   macro avg       0.78      0.73      0.73       150\n",
            "weighted avg       0.78      0.73      0.73       150\n",
            "\n",
            "Validation Accuracy: 73.33%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 13/15 Started\n",
            "  Batch [10/44] - Loss: 0.6683\n",
            "  Batch [20/44] - Loss: 0.7180\n",
            "  Batch [30/44] - Loss: 0.5886\n",
            "  Batch [40/44] - Loss: 0.6273\n",
            "  Batch [44/44] - Loss: 0.5503\n",
            "\n",
            "Epoch 13 Complete - Loss: 0.6573, Train Accuracy: 92.71%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.60      0.72        30\n",
            "           1       0.60      1.00      0.75        30\n",
            "           2       0.65      0.73      0.69        30\n",
            "           3       1.00      0.40      0.57        30\n",
            "           4       0.74      0.83      0.78        30\n",
            "\n",
            "    accuracy                           0.71       150\n",
            "   macro avg       0.78      0.71      0.70       150\n",
            "weighted avg       0.78      0.71      0.70       150\n",
            "\n",
            "Validation Accuracy: 71.33%\n",
            "\n",
            "Epoch 14/15 Started\n",
            "  Batch [10/44] - Loss: 0.6537\n",
            "  Batch [20/44] - Loss: 0.5177\n",
            "  Batch [30/44] - Loss: 0.5824\n",
            "  Batch [40/44] - Loss: 0.7824\n",
            "  Batch [44/44] - Loss: 0.7948\n",
            "\n",
            "Epoch 14 Complete - Loss: 0.6400, Train Accuracy: 92.57%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.63      0.73        30\n",
            "           1       0.54      0.97      0.69        30\n",
            "           2       0.67      0.67      0.67        30\n",
            "           3       1.00      0.33      0.50        30\n",
            "           4       0.74      0.83      0.78        30\n",
            "\n",
            "    accuracy                           0.69       150\n",
            "   macro avg       0.76      0.69      0.67       150\n",
            "weighted avg       0.76      0.69      0.67       150\n",
            "\n",
            "Validation Accuracy: 68.67%\n",
            "\n",
            "Epoch 15/15 Started\n",
            "  Batch [10/44] - Loss: 0.5752\n",
            "  Batch [20/44] - Loss: 0.4793\n",
            "  Batch [30/44] - Loss: 0.5611\n",
            "  Batch [40/44] - Loss: 0.5401\n",
            "  Batch [44/44] - Loss: 0.6003\n",
            "\n",
            "Epoch 15 Complete - Loss: 0.6061, Train Accuracy: 92.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.63      0.75        30\n",
            "           1       0.67      1.00      0.80        30\n",
            "           2       0.74      0.67      0.70        30\n",
            "           3       0.91      0.70      0.79        30\n",
            "           4       0.76      0.87      0.81        30\n",
            "\n",
            "    accuracy                           0.77       150\n",
            "   macro avg       0.80      0.77      0.77       150\n",
            "weighted avg       0.80      0.77      0.77       150\n",
            "\n",
            "Validation Accuracy: 77.33%\n",
            "Best model saved!\n",
            "\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evalutaitng the trained models metrics for each class\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "test_acc = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxsn__b-afrR",
        "outputId": "95e4d8e7-83ae-4f8c-c187-81f31506e6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.60      0.72        30\n",
            "           1       0.52      0.93      0.67        30\n",
            "           2       0.75      0.50      0.60        30\n",
            "           3       0.96      0.77      0.85        30\n",
            "           4       0.78      0.83      0.81        30\n",
            "\n",
            "    accuracy                           0.73       150\n",
            "   macro avg       0.78      0.73      0.73       150\n",
            "weighted avg       0.78      0.73      0.73       150\n",
            "\n",
            "Test Accuracy: 72.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model with Precision, Recall, and F1 score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Function for evalutating the model metrics and confusion matrix\n",
        "def compute_metrics(model, test_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    #Getting the precision, recall and f1 scores of the model\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    #Printing scores\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    #Confusion matrix with heatmap\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function on the test set\n",
        "compute_metrics(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "XPxp3M7qsNmU",
        "outputId": "2bad5279-aee9-4d60-c4a2-cf49c78c5d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7816\n",
            "Recall: 0.7267\n",
            "F1 Score: 0.7290\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the final trained model\n",
        "torch.save(model.state_dict(), \"final_model.pth\")"
      ],
      "metadata": {
        "id": "OP-mKVX5ah1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "#Define the augmentation transforms\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=1.0, interpolation=3),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.GaussianBlur(3),\n",
        "    transforms.RandomRotation(45)\n",
        "])\n",
        "\n",
        "#Define function to save augmented images\n",
        "def save_augmented_images(src_dir, dst_dir, num_augmentations=9):\n",
        "    if not os.path.exists(dst_dir):\n",
        "        os.makedirs(dst_dir)\n",
        "\n",
        "    #Iterate over the classes\n",
        "    for class_name in os.listdir(src_dir):\n",
        "        class_path = os.path.join(src_dir, class_name)\n",
        "\n",
        "        #Check if the folder is a class folder\n",
        "        if os.path.isdir(class_path):\n",
        "            class_save_path = os.path.join(dst_dir, class_name)\n",
        "            if not os.path.exists(class_save_path):\n",
        "                os.makedirs(class_save_path)\n",
        "\n",
        "            #Print class name for debugging\n",
        "            print(f\"Processing class: {class_name}\")\n",
        "\n",
        "            #Count the number of images in the class folder\n",
        "            image_count = 0\n",
        "            for img_name in os.listdir(class_path):\n",
        "                #Check for image file extensions\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.JPG', '.JPEG')):\n",
        "                    image_count += 1\n",
        "\n",
        "            #Print the number of images found in the class folder\n",
        "            print(f\"  Found {image_count} image(s) in class {class_name}\")\n",
        "\n",
        "            #Iterate over each image in the class folder\n",
        "            for img_name in os.listdir(class_path):\n",
        "                img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "                #Only process image files\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.JPG', '.JPEG')):\n",
        "                    try:\n",
        "                        img = Image.open(img_path).convert(\"RGB\")\n",
        "                        #Save the original image\n",
        "                        original_img_path = os.path.join(class_save_path, img_name)\n",
        "                        img.save(original_img_path)\n",
        "\n",
        "                        for i in range(num_augmentations):\n",
        "                            #Apply augmentations\n",
        "                            augmented_img = augmentation_transforms(img)\n",
        "\n",
        "                            #Save the augmented image with a unique name\n",
        "                            augmented_img.save(os.path.join(class_save_path, f\"{img_name.split('.')[0]}_aug_{i}.jpg\"))\n",
        "\n",
        "                        #Debugging output to check that images are being processed\n",
        "                        print(f\"  Processed {img_name}, saved 1 original and {num_augmentations} augmented images.\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing image {img_name}: {e}\")\n",
        "\n",
        "#Define paths to your original dataset on Google Drive\n",
        "base_path = '/content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset'\n",
        "\n",
        "#Define the paths for the validation and test datasets\n",
        "val_path = os.path.join(base_path, 'val')\n",
        "test_path = os.path.join(base_path, 'test')\n",
        "\n",
        "#Define the paths to save augmented datasets\n",
        "augmented_val_path = os.path.join(base_path, 'augmented_val')\n",
        "augmented_test_path = os.path.join(base_path, 'augmented_test')\n",
        "\n",
        "#Apply augmentations\n",
        "save_augmented_images(val_path, augmented_val_path, num_augmentations=9)\n",
        "save_augmented_images(test_path, augmented_test_path, num_augmentations=9)\n",
        "\n",
        "print(f\"Augmented validation images saved to: {augmented_val_path}\")\n",
        "print(f\"Augmented test images saved to: {augmented_test_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrWCoEpyvG9m",
        "outputId": "b2e4d0f3-218f-43de-9921-215a4785d52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing class: WickedCovers\n",
            "  Found 3 image(s) in class WickedCovers\n",
            "  Processed wicked1.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed wicked14.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed wicked10.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: TasteCovers\n",
            "  Found 3 image(s) in class TasteCovers\n",
            "  Processed taste10.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed taste9.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed taste14.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: MidnightsCovers\n",
            "  Found 3 image(s) in class MidnightsCovers\n",
            "  Processed midnights20.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed midnights18.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed midnights7.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: WhitneyCovers\n",
            "  Found 3 image(s) in class WhitneyCovers\n",
            "  Processed whitney6.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed whitney7.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed whitney16.jpeg, saved 1 original and 9 augmented images.\n",
            "Processing class: RumoursCovers\n",
            "  Found 3 image(s) in class RumoursCovers\n",
            "  Processed rumours.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed rumours5.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed rumours4.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: WickedCovers\n",
            "  Found 3 image(s) in class WickedCovers\n",
            "  Processed wicked18.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed wicked4.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed wicked.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: TasteCovers\n",
            "  Found 3 image(s) in class TasteCovers\n",
            "  Processed taste15.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed taste19.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed taste18.jpeg, saved 1 original and 9 augmented images.\n",
            "Processing class: MidnightsCovers\n",
            "  Found 3 image(s) in class MidnightsCovers\n",
            "  Processed midnights14.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed midnights11.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed midnights9.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: WhitneyCovers\n",
            "  Found 3 image(s) in class WhitneyCovers\n",
            "  Processed whitney17.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed whitney19.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed whitney14.jpg, saved 1 original and 9 augmented images.\n",
            "Processing class: RumoursCovers\n",
            "  Found 3 image(s) in class RumoursCovers\n",
            "  Processed rumours6.jpeg, saved 1 original and 9 augmented images.\n",
            "  Processed rumours16.jpg, saved 1 original and 9 augmented images.\n",
            "  Processed rumours11.jpg, saved 1 original and 9 augmented images.\n",
            "Augmented validation images saved to: /content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset/augmented_val\n",
            "Augmented test images saved to: /content/drive/MyDrive/FinalYearProject/ClassificationModels/AugmentedDataset/augmented_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vinyl Record Classifier Model"
      ],
      "metadata": {
        "id": "xrr0ZecJQVuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random"
      ],
      "metadata": {
        "id": "xW-MgOWbRYJS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Define dataset paths\n",
        "dataset_path = \"/content/drive/MyDrive/FinalYearProject/ClassificationModels/RecordDataset\"\n",
        "output_path = \"/content/drive/MyDrive/FinalYearProject/ClassificationModels/RecordSplitDataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jv5dEQpRbWO",
        "outputId": "57f1973c-5aa2-4ac3-8668-4108971190c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create train, val, test directories\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        os.makedirs(os.path.join(output_path, split, class_name), exist_ok=True)"
      ],
      "metadata": {
        "id": "Tflqq-NBSzAV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set split ratios\n",
        "split_ratio = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15}\n",
        "\n",
        "#Organising the record dataset intro train, test, val splits and into their classes\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    images = os.listdir(os.path.join(dataset_path, class_name))\n",
        "    random.shuffle(images)\n",
        "\n",
        "    train_split = int(split_ratio[\"train\"] * len(images))\n",
        "    val_split = int(split_ratio[\"val\"] * len(images))\n",
        "\n",
        "    train_files = images[:train_split]\n",
        "    val_files = images[train_split : train_split + val_split]\n",
        "    test_files = images[train_split + val_split:]\n",
        "\n",
        "    for file in train_files:\n",
        "        shutil.copy(os.path.join(dataset_path, class_name, file),\n",
        "                    os.path.join(output_path, \"train\", class_name, file))\n",
        "\n",
        "    for file in val_files:\n",
        "        shutil.copy(os.path.join(dataset_path, class_name, file),\n",
        "                    os.path.join(output_path, \"val\", class_name, file))\n",
        "\n",
        "    for file in test_files:\n",
        "        shutil.copy(os.path.join(dataset_path, class_name, file),\n",
        "                    os.path.join(output_path, \"test\", class_name, file))"
      ],
      "metadata": {
        "id": "2lQvx9hKS4pA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "0edca35d-cb7d-4cbb-d6ce-728fc66e5b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bf1312ee9916>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         shutil.copy(os.path.join(dataset_path, class_name, file),\n\u001b[0m\u001b[1;32m     17\u001b[0m                     os.path.join(output_path, \"train\", class_name, file))\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations and Augmnetations"
      ],
      "metadata": {
        "id": "3clp9gKfYRVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "OfFb3U7XYUr8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "#Defining transformations for validation and test\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "a8Cvp-FCYXIc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(output_path, \"train\"), transform=train_transforms)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(output_path, \"val\"), transform=val_test_transforms)\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(output_path, \"test\"), transform=val_test_transforms)"
      ],
      "metadata": {
        "id": "7Rm5PvmFYZl_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dataloaders\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "LQZwM23WYc3o"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficentNet Model"
      ],
      "metadata": {
        "id": "wqbPqWdQY1lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "rTqHkf9IY5JA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EV4cxIKdY-T_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load EfficientNet-B0\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "#Modify classifier layer\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "num_classes = 5\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(num_ftrs, num_classes)\n",
        ")\n",
        "\n",
        "#Move model to device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "kpuL7zUOZBA6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "iLvP9F9AZKze"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "metadata": {
        "id": "2lNRgx3HZOzd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "Mp7lTNdEZS1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import sys"
      ],
      "metadata": {
        "id": "7VPUjceqbnff"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "best_val_acc = 0.0"
      ],
      "metadata": {
        "id": "yXDM2XtebLu6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to evaluate model accuracy\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "OAynPM6NZVww"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the training function with your structure\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Started\")\n",
        "        model.train()  #Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            #Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            #Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            #Show progress every few batches\n",
        "            if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_loader) - 1:\n",
        "                print(f\"  Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        #Epoch summary\n",
        "        train_acc = correct / total\n",
        "        print(f\"\\nEpoch {epoch+1} Complete - Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc*100:.2f}%\")\n",
        "\n",
        "        #Validate the model\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "        #Save the best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_vinyl_model.pth\")\n",
        "            print(\"Best model saved!\")"
      ],
      "metadata": {
        "id": "ht1wggM_bJXo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAaWK72QbN9X",
        "outputId": "67af679a-34a7-4e10-fd0b-bf0782b29426"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10 Started\n",
            "  Batch [5/5] - Loss: 1.3662\n",
            "\n",
            "Epoch 1 Complete - Loss: 1.5370, Train Accuracy: 29.58%\n",
            "Validation Accuracy: 33.33%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 2/10 Started\n",
            "  Batch [5/5] - Loss: 1.2192\n",
            "\n",
            "Epoch 2 Complete - Loss: 1.3654, Train Accuracy: 53.52%\n",
            "Validation Accuracy: 53.33%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 3/10 Started\n",
            "  Batch [5/5] - Loss: 1.0518\n",
            "\n",
            "Epoch 3 Complete - Loss: 1.1873, Train Accuracy: 73.24%\n",
            "Validation Accuracy: 60.00%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 4/10 Started\n",
            "  Batch [5/5] - Loss: 1.2268\n",
            "\n",
            "Epoch 4 Complete - Loss: 1.1208, Train Accuracy: 74.65%\n",
            "Validation Accuracy: 93.33%\n",
            "Best model saved!\n",
            "\n",
            "Epoch 5/10 Started\n",
            "  Batch [5/5] - Loss: 0.9693\n",
            "\n",
            "Epoch 5 Complete - Loss: 0.9739, Train Accuracy: 85.92%\n",
            "Validation Accuracy: 93.33%\n",
            "\n",
            "Epoch 6/10 Started\n",
            "  Batch [5/5] - Loss: 0.9524\n",
            "\n",
            "Epoch 6 Complete - Loss: 0.8564, Train Accuracy: 90.14%\n",
            "Validation Accuracy: 86.67%\n",
            "\n",
            "Epoch 7/10 Started\n",
            "  Batch [5/5] - Loss: 0.8151\n",
            "\n",
            "Epoch 7 Complete - Loss: 0.7419, Train Accuracy: 95.77%\n",
            "Validation Accuracy: 93.33%\n",
            "\n",
            "Epoch 8/10 Started\n",
            "  Batch [5/5] - Loss: 0.8150\n",
            "\n",
            "Epoch 8 Complete - Loss: 0.6946, Train Accuracy: 95.77%\n",
            "Validation Accuracy: 93.33%\n",
            "\n",
            "Epoch 9/10 Started\n",
            "  Batch [5/5] - Loss: 0.9251\n",
            "\n",
            "Epoch 9 Complete - Loss: 0.6057, Train Accuracy: 97.18%\n",
            "Validation Accuracy: 93.33%\n",
            "\n",
            "Epoch 10/10 Started\n",
            "  Batch [5/5] - Loss: 0.6059\n",
            "\n",
            "Epoch 10 Complete - Loss: 0.4933, Train Accuracy: 97.18%\n",
            "Validation Accuracy: 93.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "#Function to evaluate the model\n",
        "def evaluate_model(model, dataloader, class_names=None, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    #Compute metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    #Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    if class_names is None:\n",
        "        class_names = [str(i) for i in range(len(cm))]\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "rwRKu6LSLTSM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the model's state dict\n",
        "torch.save(model.state_dict(), 'vinyl_record_model.pth')"
      ],
      "metadata": {
        "id": "Ar_JdpQndELT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}